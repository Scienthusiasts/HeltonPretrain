# coding=utf-8
import os
import json
import torch
import logging
import time
import datetime
import argparse
from torch import nn
from logging import Logger
import torch.optim as optim
from functools import partial
from torch.utils.data import DataLoader
from timm.scheduler import CosineLRScheduler
from torch.utils.tensorboard import SummaryWriter
from typing import List, Dict, Optional
# å¤šå¡å¹¶è¡Œè®­ç»ƒ:
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP





class ArgsHistory():
    """è®°å½•trainæˆ–valè¿‡ç¨‹ä¸­çš„ä¸€äº›å˜é‡(æ¯”å¦‚ loss, lrç­‰) ä»¥åŠtensorboard
       ä»¥å­—å…¸å½¢å¼å­˜å‚¨, ä¾‹:{'loss1':[...], 'loss2':[...], ...}
    """
    def __init__(self, json_save_dir, mode):
        # NOTE:å¤šå¡
        if mode in ['train', 'eval'] or (mode=='train_ddp' and dist.get_rank() == 0):
            # tensorboard å¯¹è±¡
            self.tb_writer = SummaryWriter(log_dir=json_save_dir)
        self.json_save_dir = json_save_dir
        # æ‰€æœ‰å˜é‡è®°å½•åœ¨self.args_history_dictä¸­
        self.args_history_dict = {}


    def record(self, key, value):
        '''æ›´æ–°å˜é‡çš„è®°å½•, å˜é‡ä¸ºkey, æ–°å¢çš„è®°å½•ä¸ºvalue
        Args:
            - key:   è¦è®°å½•çš„å½“å‰å˜é‡çš„åå­—
            - value: è¦è®°å½•çš„å½“å‰å˜é‡çš„æ•°å€¼
            
        Returns:
            None
        '''
        # å¯èƒ½å­˜åœ¨jsonæ ¼å¼ä¸æ”¯æŒçš„ç±»å‹, å› æ­¤ç»Ÿä¸€è½¬æˆfloatç±»å‹
        value = float(value)
        # å¦‚æœæ—¥å¿—ä¸­è¿˜æ²¡æœ‰è¿™ä¸ªå˜é‡ï¼Œåˆ™æ–°å»º
        if key not in self.args_history_dict.keys():
            self.args_history_dict[key] = []
        # æ›´æ–°history dict
        self.args_history_dict[key].append(value)
        # é¡ºä¾¿æ›´æ–°tensorboard
        self.tb_writer.add_scalar(key, value, len(self.args_history_dict[key]))


    def saveRecord(self):
        '''ä»¥jsonæ ¼å¼ä¿å­˜è®°å½•
        '''
        if not os.path.isdir(self.json_save_dir):os.makedirs(self.json_save_dir) 
        json_save_path = os.path.join(self.json_save_dir, 'args_history.json')
        # ä¿å­˜
        with open(json_save_path, 'w') as json_file:
            json.dump(self.args_history_dict, json_file)


    def loadRecord(self, json_load_dir):
        '''å¯¼å…¥ä¸Šä¸€æ¬¡è®­ç»ƒæ—¶çš„args(ä¸€èˆ¬ç”¨äºresume)
        '''
        json_path = os.path.join(json_load_dir, 'args_history.json')
        with open(json_path, "r", encoding="utf-8") as json_file:
            self.args_history_dict = json.load(json_file)








class ModelInfoLogger:
    """æ¨¡å‹ä¿¡æ¯è®°å½•å™¨ï¼Œç”¨äºç”Ÿæˆç±»ä¼¼MMDetectionçš„æ¨¡å‹å‚æ•°è¡¨æ ¼"""
    
    @staticmethod
    def get_parameter_info(model: nn.Module, optimizer:optim.Optimizer) -> List[Dict]:
        """è·å–æ¨¡å‹å‚æ•°ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            optimizer: ä¼˜åŒ–å™¨å®ä¾‹ï¼ˆç”¨äºè·å–å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ï¼‰
            
        Returns:
            å‚æ•°ä¿¡æ¯åˆ—è¡¨
        """
        param_info = []
        
        # åˆ›å»ºå‚æ•°åˆ°ä¼˜åŒ–å™¨å‚æ•°ç»„çš„æ˜ å°„
        param_to_group = {}
        for group_idx, param_group in enumerate(optimizer.param_groups):
            for param in param_group['params']:
                param_to_group[param] = group_idx
        
        for name, param in model.named_parameters():
            # è·å–å‚æ•°å½¢çŠ¶
            shape = "X".join(str(dim) for dim in param.shape)
            
            # è·å–å‚æ•°å€¼èŒƒå›´
            if param.numel() > 0:
                min_val = param.data.min().item()
                max_val = param.data.max().item()
                value_scale = f"Min:{min_val:.3f} Max:{max_val:.3f}"
            else:
                value_scale = "Min:0.000 Max:0.000"
            
            # åˆ¤æ–­æ˜¯å¦è¢«ä¼˜åŒ–ï¼ˆé€šå¸¸æ˜¯å¯è®­ç»ƒå‚æ•°ï¼‰
            optimized = "Y" if param.requires_grad else "N"
            
            # é»˜è®¤å€¼
            lr = 0.001
            wd = 0.0
            
            # ä»ä¼˜åŒ–å™¨ä¸­è·å–å®é™…çš„å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡
            if param in param_to_group:
                group_idx = param_to_group[param]
                param_group = optimizer.param_groups[group_idx]
                lr = param_group.get('lr', lr)
                wd = param_group.get('weight_decay', wd)
            
            param_info.append({
                'name': name,
                'optimized': optimized,
                'shape': shape,
                'value_scale': value_scale,
                'lr': lr,
                'wd': wd
            })
        
        return param_info
    

    @staticmethod
    def create_model_table(param_info: List[Dict], max_name_width: int = 80, 
                        max_shape_width: int = 15) -> str:
        """åˆ›å»ºæ¨¡å‹å‚æ•°è¡¨æ ¼
        Args:
            param_info: å‚æ•°ä¿¡æ¯åˆ—è¡¨
            max_name_width: åç§°åˆ—æœ€å¤§å®½åº¦
            max_shape_width: å½¢çŠ¶åˆ—æœ€å¤§å®½åº¦
            
        Returns:
            æ ¼å¼åŒ–åçš„è¡¨æ ¼å­—ç¬¦ä¸²
        """
        if not param_info:
            return "No parameters found."
        
        # è¡¨æ ¼åˆ—å®šä¹‰
        columns = [
            {'name': 'Parameter Name', 'key': 'name', 'width': max_name_width},
            {'name': 'Optimized', 'key': 'optimized', 'width': 10},
            {'name': 'Shape', 'key': 'shape', 'width': max_shape_width},
            {'name': 'Value Scale [Min, Max]', 'key': 'value_scale', 'width': 25},
            {'name': 'Init Lr', 'key': 'lr', 'width': 8},
            {'name': 'Wd', 'key': 'wd', 'width': 8}
        ]
        
        # è®¡ç®—è¡¨æ ¼æ€»å®½åº¦
        table_width = sum(col['width'] for col in columns) + len(columns) * 3 + 1
        
        # åˆ›å»ºè¡¨æ ¼æ ‡é¢˜
        title = "Model Information"
        title_line = f"+{'-' * (table_width - 2)}+"
        title_row = f"|{title:^{table_width - 2}}|"
        # åˆ›å»ºè¡¨å¤´åˆ†éš”çº¿
        header_separator = "+".join(["-" * (col['width'] + 2) for col in columns])
        header_separator = f"+{header_separator}+"
        # åˆ›å»ºè¡¨å¤´
        header = "|"
        for col in columns:
            header += f" {col['name']:^{col['width']}} |"
        # æ„å»ºè¡¨æ ¼
        table_lines = []
        table_lines.append(title_line)
        table_lines.append(title_row)
        table_lines.append(header_separator)
        table_lines.append(header)
        table_lines.append(header_separator)
        
        # æ·»åŠ æ•°æ®è¡Œï¼ˆå±…ä¸­å¯¹é½ï¼‰
        for info in param_info:
            row = "|"
            for col in columns:
                value = info[col['key']]
                if col['key'] in ['lr', 'wd']:
                    formatted_value = f"{value:.5f}" if isinstance(value, float) else str(value)
                else:
                    formatted_value = str(value)
                
                if len(formatted_value) > col['width']:
                    formatted_value = formatted_value[:col['width']-3] + "..."
                
                row += f" {formatted_value:^{col['width']}} |"
            table_lines.append(row)
        
        table_lines.append(header_separator)
        
        return "\n".join(table_lines)

    
    @staticmethod
    def get_model_summary(model: nn.Module) -> Dict:
        """è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            
        Returns:
            æ¨¡å‹æ‘˜è¦ä¿¡æ¯å­—å…¸
        """
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'non_trainable_parameters': total_params - trainable_params,
            'parameter_memory_MB': total_params * 4 / (1024 ** 2)  # å‡è®¾float32ï¼Œ4å­—èŠ‚/å‚æ•°
        }














class RunnerLogger:
    """æ—¥å¿—è®°å½•/æ‰“å°ç›¸å…³
    """
    def __init__(self, mode:str, log_dir:str, log_interval:int, eval_interval:int, batch_num:int, total_epoch:int):
        '''ç”Ÿæˆloggeræ—¥å¿—å¯¹è±¡ç”¨äºåç»­æ‰“å°å’Œè®°å½•
            Args:
                mode:          ç½‘ç»œæ¨¡å‹
                log_dir:       æ—¥å¿—æ–‡ä»¶ä¿å­˜ç›®å½•
                log_interval:  æ—¥å¿—æ‰“å°é—´éš”(å‡ ä¸ªiteræ‰“å°ä¸€æ¬¡)
                eval_interval: æ¯éš”å¤šå°‘epochè¯„ä¼°ä¸€æ¬¡
                batch_num:     ä¸€ä¸ªepochåŒ…å«çš„batchæ•°é‡
                total_epoch:   æ€»è®­ç»ƒepoch
        '''
        self.mode = mode
        self.log_interval = log_interval
        self.eval_interval = eval_interval
        self.logger = logging.getLogger('runer')
        self.batch_num = batch_num
        self.logger.setLevel(level=logging.DEBUG)
        self.log_dir = log_dir
        self.total_iters = total_epoch * self.batch_num
        self.last_time = time.time()
        self.interval_time = 0
        # æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter('%(asctime)s: %(message)s')
        # å†™å…¥æ–‡ä»¶çš„æ—¥å¿—
        self.log_dir = os.path.join(self.log_dir, f"{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}_{mode}")
        # æ—¥å¿—æ–‡ä»¶ä¿å­˜è·¯å¾„
        self.log_save_path = os.path.join(self.log_dir, f"{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}_{mode}.log")
        if not os.path.isdir(self.log_dir):os.makedirs(self.log_dir)
        # NOTE:å¤šå¡
        # åªåœ¨ local_rank ä¸º 0 çš„è¿›ç¨‹ä¸­è®¾ç½®æ—¥å¿—è®°å½•
        if mode in ['train', 'eval'] or (mode == 'train_ddp' and dist.get_rank() == 0):
            # è®°å½•åˆ°logæ–‡ä»¶ä¸­çš„æ—¥å¿—
            file_handler = logging.FileHandler(self.log_save_path, encoding="utf-8", mode="a")
            file_handler.setLevel(level=logging.INFO)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
            # ç»ˆç«¯è¾“å‡ºçš„æ—¥å¿—
            stream_handler = logging.StreamHandler()
            stream_handler.setLevel(logging.INFO)
            stream_handler.setFormatter(formatter)
            self.logger.addHandler(stream_handler)
        # å¯¹äºéä¸»è¿›ç¨‹ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªç©ºçš„æ—¥å¿—å¤„ç†å™¨æ¥å¿½ç•¥æ—¥å¿—è®°å½•
        else:
            self.logger.addHandler(logging.NullHandler())

        self.argsHistory = ArgsHistory(self.log_dir, self.mode)
        self.model_info_logger = ModelInfoLogger()



    def log_model_info(self, model: nn.Module, optimizer:optim.Optimizer):
        """æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯è¡¨æ ¼
            Args:
                model: æ¨¡å‹å®ä¾‹
                optimizer: ä¼˜åŒ–å™¨å®ä¾‹ï¼ˆç”¨äºè·å–çœŸå®çš„å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ï¼‰
        """
        # ä¿å­˜åŸ formatter
        handlers = self.logger.handlers
        original_formatters = [h.formatter for h in handlers]
        # ä¸´æ—¶æ›¿æ¢ä¸ºæ—  asctime çš„ formatter
        simple_formatter = logging.Formatter('%(message)s')
        for h in handlers:
            h.setFormatter(simple_formatter)
        try:
            # è·å–å‚æ•°ä¿¡æ¯
            param_info = self.model_info_logger.get_parameter_info(model, optimizer)
            model_table = self.model_info_logger.create_model_table(param_info)
            summary = self.model_info_logger.get_model_summary(model)

            for line in model_table.split('\n'):
                self.logger.info(line)
            self.logger.info(f"ğŸ§© Model Info:")
            self.logger.info(f"  â¤ Total Parameters: {summary['total_parameters']:,}")
            self.logger.info(f"  â¤ Trainable Parameters: {summary['trainable_parameters']:,}")
            self.logger.info(f"  â¤ Non-trainable Parameters: {summary['non_trainable_parameters']:,}")
            self.logger.info(f"  â¤ Estimated Memory Usage: {summary['parameter_memory_MB']:.2f} MB \n")
        finally:
            # æ¢å¤åŸ formatter
            for h, f in zip(handlers, original_formatters):
                h.setFormatter(f)



    def train_iter_log_printer(self, step:int, epoch:int, optimizer:optim, losses:dict):
        """è®­ç»ƒ/éªŒè¯è¿‡ç¨‹ä¸­è®°å½•/æ‰“å°æ—¥å¿—
            Args: 
                optimizer:    ä¼˜åŒ–å™¨å®ä¾‹(ç”¨äºæ‰“å°å½“å‰å­¦ä¹ ç‡)
                step:         å½“å‰è¿­ä»£åˆ°ç¬¬å‡ ä¸ªbatch
                epoch:        å½“å‰è¿­ä»£åˆ°ç¬¬å‡ ä¸ªepoch
                losses:       å½“å‰batchçš„loss(å­—å…¸)
            æ‰“å°æ•ˆæœ:
                2025-09-22 22:54:39,656: Epoch(train) [2][50/59]  lr: 0.000990  total_loss: 1.14722  cls_loss: 1.14722  acc: 0.82812  
                2025-09-22 22:54:42,102: Epoch(train) [2][55/59]  lr: 0.000990  total_loss: 1.08477  cls_loss: 1.08477  acc: 0.73438 
        """   
        # è®°å½•æ¯æ¬¡iterè€—æ—¶
        self.interval_time = time.time() - self.last_time
        self.last_time = time.time()
        eta_hours = (self.total_iters - step) * self.interval_time / 3600
        eta_mins = (eta_hours - int(eta_hours)) * 60
        '''è®°å½•'''
        current_lr = optimizer.param_groups[0]['lr']
        self.argsHistory.record('lr', current_lr)
        # è®°å½•æ‰€æœ‰æŸå¤±:
        for loss_name, loss_value in losses.items():
            self.argsHistory.record(loss_name, loss_value.item())
        '''æ‰“å°'''
        # æ¯é—´éš”log_intervalä¸ªiteræ‰æ‰“å°ä¸€æ¬¡
        if step % self.log_interval != 0:
            return
        # å³å¯¹é½, æ‰“å°æ›´ç¾è§‚
        batch_idx = '{:>{}}'.format(step, len(f"{self.batch_num}"))
        log = ("Epoch(train) [%d][%s/%d] eta: %02d:%02d  lr: %8f  ") % (epoch, batch_idx, self.batch_num, int(eta_hours), int(eta_mins), current_lr)
        for loss_name, loss_value in losses.items():
            loss_log = (loss_name+": %.5f  " % (loss_value.item()))
            log += loss_log
        self.logger.info(log)



    def train_epoch_log_printer(self, epoch, evaluations, flag_metric_name):
        """è®­ç»ƒ/éªŒè¯è¿‡ç¨‹ä¸­è®°å½•/æ‰“å°æ—¥å¿—
            Args: 
                epoch:        å½“å‰è¿­ä»£åˆ°ç¬¬å‡ ä¸ªepoch
                acc:          å‡†ç¡®ç‡(æŒ‡æ ‡)
                mAP:          mAP(æŒ‡æ ‡)
                mF1_score:    mF1_score(æŒ‡æ ‡)
            æ‰“å°æ•ˆæœ:
            2025-09-22 22:56:43,934: ==================================================================================================================
            2025-09-22 22:56:43,934: Epoch(valid) [1]  val_acc: 0.50286  val_mAP: 0.36976  val_mF1: 0.40191  best_val_epoch: 1  best_val_acc: 0.50286
            2025-09-22 22:56:43,934: ==================================================================================================================
        """        
        '''è®°å½•è¯„ä¼°æŒ‡æ ‡'''
        for metric_name, metric_value in evaluations.items():
            self.argsHistory.record(metric_name, metric_value)
        # ä»¥jsonæ ¼å¼ä¿å­˜args
        self.argsHistory.saveRecord()

        '''æ‰“å°'''
        # æ‰¾åˆ°æœ€é«˜å‡†ç¡®ç‡å¯¹åº”çš„epoch
        flag_metric_list = self.argsHistory.args_history_dict[flag_metric_name]
        best_epoch = flag_metric_list.index(max(flag_metric_list)) + 1
        best_flag_metric_val = max(flag_metric_list)
        # æ‰“å°æŒ‡æ ‡
        self.logger.info('=' * 150)
        log = ("Epoch(valid) [%d]  ") % (epoch)
        for metric_name, metric_value in evaluations.items():
            loss_log = (metric_name+": %.5f  " % (metric_value))
            log += loss_log
        log += (f"best_val_epoch: {best_epoch * self.eval_interval}  best_{flag_metric_name}: {best_flag_metric_val:.5f}")
        self.logger.info(log)
        self.logger.info('=' * 150)

