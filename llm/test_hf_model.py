import argparse
import random
import warnings
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from heltonx.utils.utils import seed_everything



def init_model(device, weight_dir):
    """åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨(åŠ è½½huggingface, transformeråº“çš„æ ¼å¼çš„æ¨¡å‹)
    """
    # AutoTokenizer ä¼šæ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„åˆ†è¯è§„åˆ™ï¼ˆä¾‹å¦‚BPEã€SentencePieceï¼‰
    tokenizer = AutoTokenizer.from_pretrained(weight_dir)
    # # åŠ è½½è¯­è¨€æ¨¡å‹, ä½¿ç”¨transformeråº“è‡ªåŠ¨è¿›è¡Œé…ç½®(ç¬¦åˆç°æœ‰å¼€æºå¤§æ¨¡å‹çš„æ ‡å‡†, å¯ä»¥ç›´æ¥å¯¼å…¥huggingfaveä¸Šçš„llmæ¨¡å‹)
    # trust_remote_code=True è¡¨ç¤ºå…è®¸åŠ è½½è¿œç¨‹ä»“åº“ä¸­è‡ªå®šä¹‰çš„æ¨¡å‹ä»£ç ï¼ˆæœ‰æ—¶éå®˜æ–¹æ¨¡å‹éœ€è¦ï¼‰
    model = AutoModelForCausalLM.from_pretrained(weight_dir, trust_remote_code=True)
    print(f'æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')
    return model.eval().to(device), tokenizer



def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # load_from = './ckpts/hugging_face/Qwen-0.6B'
    load_from = './ckpts/hugging_face/MiniMind2-R1'
    historys = 0
    seed_everything(42) 

    # åˆå§‹åŒ–å¯¹è¯å­˜å‚¨åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ä¸Šä¸‹æ–‡å†å²
    conversation = []
    model, tokenizer = init_model(device, load_from)
    # åˆ›å»ºæµå¼è¾“å‡ºå™¨ï¼ˆè¾¹ç”Ÿæˆè¾¹æ‰“å°ï¼‰
    # skip_prompt=True è¡¨ç¤ºä¸é‡å¤æ‰“å°ç”¨æˆ·è¾“å…¥
    # skip_special_tokens=True è¡¨ç¤ºè·³è¿‡ <bos>ã€<eos>ã€<pad> ç­‰ç‰¹æ®Šç¬¦å·
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    prompt_iter = iter(lambda: input('ğŸ‘¶: '), '')
    for prompt in prompt_iter:
        # ä¿ç•™æœ€è¿‘çš„å†å²å¯¹è¯ï¼ˆå¦‚æœè®¾ç½®äº†historysï¼‰
        conversation = conversation[-historys:] if historys else []
        # åœ¨æ¯æ¬¡æ„é€  templates å‰éƒ½ä¿è¯ system å­˜åœ¨ä¸€æ¬¡
        system_prompt = {
            "role": "system", 
            "content": "ä½ æ˜¯ä¸€ä¸ªç”±YHTå¼€å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹, ä½ éœ€è¦è®°ä½ä½ çš„åå­—å«åšHeltonLM, ä½ éœ€è¦å°½å¯èƒ½çš„åˆ†æç”¨æˆ·æå‡ºçš„éœ€æ±‚ï¼Œå¹¶ç»™å‡ºå®Œç¾çš„å›ç­”ï¼Œè®°ä½ï¼Œå½“é—®é¢˜æ¯”è¾ƒå¤æ‚æ—¶ä½ åº”è¯¥å°½å¯èƒ½çš„ä¸¾ä¸€ä¸ªç”ŸåŠ¨å½¢è±¡çš„ä¾‹å­"
        }
        if not conversation or conversation[0].get("role") != "system":
            conversation.insert(0, system_prompt)
        # å°†ç”¨æˆ·å½“å‰è¾“å…¥åŠ å…¥åˆ°å¯¹è¯ä¸Šä¸‹æ–‡
        conversation.append({"role": "user", "content": prompt})
        # æ„é€ å¯¹è¯æ¨¡æ¿ï¼ˆHugging Face çš„chatæ¨¡æ¿ï¼Œç”¨äºè‡ªåŠ¨æ‹¼æ¥system+user+assistantæ ¼å¼ï¼‰
        templates = {
            "conversation": conversation,
            "tokenize": False, 
            "add_generation_prompt": True,
            # æ˜¯å¦å¼€å¯CoT
            "enable_thinking": True
        }
        # å°†æ¨¡æ¿è½¬æ¢æˆå¯ä¾›æ¨¡å‹è¾“å…¥çš„å­—ç¬¦ä¸²
        inputs = tokenizer.apply_chat_template(**templates)
        # ä½¿ç”¨tokenizerå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥ï¼ˆtoken idsã€attention maskï¼‰
        # truncation=True ä¿è¯è¾“å…¥ä¸è¿‡é•¿
        inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(device)
        # æç¤ºè¾“å‡º
        print('ğŸ¤–ï¸: ', end='')
        # ä½¿ç”¨æ¨¡å‹çš„generate()æ¥å£ç”Ÿæˆæ–‡æœ¬ï¼ˆå³è‡ªå›å½’ç”Ÿæˆï¼‰
        generated_ids = model.generate(
            inputs=inputs["input_ids"],               # è¾“å…¥çš„token ids
            attention_mask=inputs["attention_mask"],  # æ³¨æ„åŠ›æ©ç ï¼ˆpaddingéƒ¨åˆ†ä¸º0ï¼‰
            max_new_tokens=8192,                      # æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample=True,                           # å¼€å¯éšæœºé‡‡æ ·ï¼ˆéè´ªå¿ƒæœç´¢ï¼‰
            streamer=streamer,                        # å®æ—¶æ‰“å°è¾“å‡º
            pad_token_id=tokenizer.pad_token_id,      # å¡«å……token id 
            eos_token_id=tokenizer.eos_token_id,      # ç»“æŸtoken id
            top_p=0.85,                               # nucleusé‡‡æ ·æ¦‚ç‡é˜ˆå€¼
            temperature=0.85,                         # æ¸©åº¦é‡‡æ ·ç³»æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
            repetition_penalty=1.0                    # é‡å¤æƒ©ç½šç³»æ•°ï¼ˆ>1æŠ‘åˆ¶é‡å¤ï¼‰
        )
        # å°†ç”Ÿæˆçš„ç»“æœè§£ç æˆæ–‡æœ¬ï¼ˆè·³è¿‡ç‰¹æ®Šç¬¦å·ï¼‰
        # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥é•¿åº¦ï¼‰
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]):],
            skip_special_tokens=True
        )
        # å°†AIçš„å›ç­”åŠ å…¥åˆ°å¯¹è¯å†å²
        conversation.append({"role": "assistant", "content": response})
        # æ‰“å°ç©ºè¡Œåˆ†éš”ä¸‹ä¸€è½®å¯¹è¯
        print('\n\n')




if __name__ == "__main__":
    main()